{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Titanic with neuronal networks"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following code was given from the lecturer and should be expanded to create a neuronal network for the titanic dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import tensorflow.keras \n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense\n",
    "model = Sequential()\n",
    "model.add(Dense(..., activation='sigmoid', batch_input_shape=(None, 4))) #We have 4 input features\n",
    "#...\n",
    "model.add(Dense(1, activation='sigmoid'))\n",
    "opt = tf.keras.optimizers.Adam(learning_rate=1e-3)\n",
    "model.compile(loss=tf.keras.losses.BinaryCrossentropy(),\n",
    "              optimizer=opt,\n",
    "              metrics=['accuracy'])\n",
    "model.summary()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Import the following packages:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import tensorflow.keras\n",
    "\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense\n",
    "from tensorflow.keras.layers import BatchNormalization\n",
    "from tensorflow.keras.layers import Normalization\n",
    "from tensorflow.keras.layers import Dropout\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "# Nice function to split a known data set to a test and trin set\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Nice function for plotting\n",
    "# Install via pip: pip install tensorflow-history-plot\n",
    "from tensorflow_history_plot import show_acc"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "\n",
    "Read the data from the csv file and show some infos about the data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = pd.read_csv(\"data/train.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data.info()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data preparation"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define the features we want to use to train our neuronal network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read data again in case you have already manipulated it\n",
    "train_data = pd.read_csv(\"data/train.csv\") \n",
    "\n",
    "# Fill missing age in train data\n",
    "train_data[\"Age\"].fillna(train_data[\"Age\"].median(skipna=True), inplace=True) \n",
    "\n",
    "# Define features we want to use\n",
    "features = [\"Pclass\", \"Survived\", \"Sex\", \"Age\"]\n",
    "\n",
    "# Get the features we want to use\n",
    "train_data = pd.get_dummies(train_data[features])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Show the info of the modified dataset (notice that there should be a equal amount of \"not null\" values for the columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data.info()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Split the data into training and test data. Therefore we will first split the data into a \"X\" and \"y\" part. The \"X\" part contains all features and the \"y\" part contains the label (survived or not). After that we will split the data into training and test data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = train_data.drop('Survived', axis=1, inplace=False)\n",
    "y = train_data['Survived']\n",
    "\n",
    "print(f'Input features shape: {X.shape}')\n",
    "print(f'Labels shape: {y.shape}')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using the `train_test_split` function from the `sklearn.model_selection` package, split the data into training and test data. \n",
    "This is an easy and fast way to split the data into training and test data. The function will return 4 values: `X_train`, `X_test`, `y_train` and `y_test`. The `X` values are the features (in our case the columns `Pclass`, `Sex`, `Age`) and the `y` values are the labels (in our case the column `Survived`)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Neural Network\n",
    "\n",
    "In the following section, the neuronal network is created and trained.\n",
    "\n",
    "First we have to convert the data into a format that can be used by the neuronal network. Therefore we will use the `asarrray` function from the `numpy` package. This function will convert the data into a numpy array."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = np.asarray(X_train).astype('float32')\n",
    "X_val = np.asarray(X_val).astype('float32')\n",
    "y_train = np.asarray(y_train).astype('float32')\n",
    "y_val = np.asarray(y_val).astype('float32')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `X` values will be converted into a 2 dimensional array..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "... and the `y` values will be converted into a 1 dimensional array."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "\n",
    "Now we will define our neuronal network. Therefore we will use the `Sequential` class from the `keras.models` package. This class will create a new neuronal network. The `Sequential` class will take a list of layers as parameter."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Sequential()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After defining our model we will add the layers to our model. Therefore we will use the `add` function. The first layer will be a `Dense` layer with the `sigmoid` activation function. The `Dense` layer will take the number of neurons as first parameter. The second parameter is the activation function. The `input_shape` parameter defines the input shape of the first layer. The input shape is the number of features. In our case the input shape is 4, because we have 4 features.\n",
    "\n",
    "The last layer will only have one neuron, because we only want to predict one value (survived or not). The activation function will be the `sigmoid` function.\n",
    "\n",
    "After defining our layers we will set the learning rate of the optimizer and compile the model. Therefore we will use the `compile` function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.add(Dense(125, activation='sigmoid', batch_input_shape=(None, 4), name=\"Layer_1\")) #We have 4 input features\n",
    "model.add(Dense(500, activation='sigmoid', name=\"Layer_2\"))\n",
    "model.add(Dense(750, activation='sigmoid', name=\"Layer_3\"))\n",
    "model.add(Dense(1, activation='sigmoid', name=\"Layer_4\"))\n",
    "\n",
    "# Ask lecturer why this is not possible???\n",
    "#model.add(Dense(1, activation=\"softmax\", name=\"Layer_5\"))\n",
    "\n",
    "opt = tf.keras.optimizers.Adam(learning_rate=1e-3)\n",
    "\n",
    "model.compile(loss=tf.keras.losses.BinaryCrossentropy(),\n",
    "              optimizer=opt,\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally we will train our model. Therefore we will use the `fit` function. The `fit` function will take the training data as first parameter, the labels as second parameter, the validation data as third parameter and the number of epochs as fourth parameter. The `fit` function will return a history object. This object contains the loss and accuracy of the training and validation data for each epoch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#history = model.fit(x, y, epochs=100, batch_size=32, validation_split=0.2, verbose=0)\n",
    "history = model.fit(X_train, y_train,\n",
    "                    validation_data=(X_val, y_val),\n",
    "                    epochs=50,\n",
    "                    batch_size=32)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The accuracy can be shwon as simple values..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.evaluate(X_val, y_val)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "... or as a nice plot using the `show_acc` function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "show_acc.plot(history)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tests"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we are ready to predict some values using the given test data set and the trained model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_data = pd.read_csv(\"data/test.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_data.info()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The data preparation is the same as before. We will convert the data into a numpy array and split the data into features and labels.\n",
    "\n",
    "Notice that the \"Survived\" column is missing in the test data set. This is because we want to predict the \"Survived\" column now."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read data again in case you have already manipulated it\n",
    "test_data = pd.read_csv(\"data/test.csv\")\n",
    "\n",
    "# Fill missing age in train data\n",
    "test_data[\"Age\"].fillna(train_data[\"Age\"].median(skipna=True), inplace=True) \n",
    "\n",
    "# Define features we want to use (again)\n",
    "features = ['Pclass', 'Sex', 'Age']\n",
    "\n",
    "##test_data.drop(['Name', 'Ticket', 'PassengerId', 'Cabin', 'Sex', 'Age', 'SibSp', 'Parch', 'Fare', 'Embarked'], axis=1, inplace=True)\n",
    "\n",
    "test_data = pd.get_dummies(test_data[features])\n",
    "\n",
    "test_data.info()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After converting the data into a numpy array we will predict the values using the `predict` function from the model. The `predict` function will take the features as parameter and will return the predicted values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_data = np.asarray(test_data).astype('float32')\n",
    "\n",
    "predictions = model.predict(test_data)\n",
    "predictions"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Because the values are not boolean values (what is needed because unless you are a zombie you are either dead or not, nothing between) we will convert the values into boolean values. Therefore we will use the `round` function from the `numpy` package.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = np.where(predictions > 0.5, 1, 0)\n",
    "predictions = predictions.flatten()\n",
    "predictions"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Export\n",
    "\n",
    "Finally we will export the predicted values into a csv file. Therefore we will use the `to_csv` function from the `pandas` package."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import time\n",
    "from datetime import datetime\n",
    "\n",
    "timestamp = time.time()\n",
    "timestamp = datetime.fromtimestamp(timestamp).strftime(\"%H_%M_%S\")\n",
    "print(timestamp)\n",
    "\n",
    "test_data = pd.read_csv(\"data/test.csv\")\n",
    "\n",
    "predictions #Enhält die Predictions 0 für Tod 1 für Survived\n",
    "output = pd.DataFrame({'PassengerId': test_data.PassengerId, 'Survived': predictions})\n",
    "\n",
    "filename = 'my_submission_nn_' + timestamp + '.csv'\n",
    "fileDir = os.path.join(\"export\", filename)\n",
    "\n",
    "output.to_csv(fileDir, index=False)\n",
    "print(\"Your submission was successfully saved!\")\n",
    "print(fileDir)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ignore stuff below"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(train_data, train_data[\"Survived\"], test_size=0.2, random_state=42)\n",
    "\n",
    "y = train_data[\"Survived\"]\n",
    "\n",
    "features = [\"Pclass\"]\n",
    "x = pd.get_dummies(train_data[features])\n",
    "\n",
    "#...\n",
    "#model.add(BatchNormalization())\n",
    "#model.add(Dropout(0.2))\n",
    "#model.add(Dense(10, activation='sigmoid'))\n",
    "#model.add(keras.layers.normalization.BatchNormalization())\n",
    "#model.add(Dropout(0.2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#model.add(Dense(1, activation='sigmoid', batch_input_shape=(None, 4))) #We have 4 input features\n",
    "model.add(Dense(10, activation='sigmoid', batch_input_shape=(1,)))\n",
    "model.add(BatchNormalization())\n",
    "model.add(Dropout(0.2))\n",
    "model.add(Dense(1024, activation='sigmoid'))\n",
    "model.add(BatchNormalization())\n",
    "model.add(Dropout(0.2))\n",
    "model.add(Dense(1, activation='softmax'))\n",
    "\n",
    "opt = tf.keras.optimizers.Adam(learning_rate=1e-3)\n",
    "\n",
    "# Compile the model\n",
    "model.compile(loss=tf.keras.losses.BinaryCrossentropy(),\n",
    "              optimizer=opt,\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "model.summary()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.7"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
